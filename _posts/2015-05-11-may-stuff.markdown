---
layout: post
title: "May stuff"
tags: python algorithm leetcode
date: May 11, 2015
---
#### Array

def twoSumII(nums, target):
    # nums is a sorted list
    
    lenOfNums = len(nums)
    index1, index2 = 0, 0
    for i in range(lenOfNums):
        index2 = lenOfNums - i - 1
        
        if nums[index2] + nums[index1] <= target:
            break
        
    # print index1, index2    
    # get the value of index1 and index2
    for index1 in range(index2 + 1):
        if nums[index1] + nums[index2] == target:
            print 'found, and the result is:'
            return index1, index2
            
        else:
            print 'not found'
            
            
nums = [2,3,4,5,7,9,11]
target = 11

print twoSumII(nums, target)

#### Big Data and PySpark

https://goo.gl/JQIqU6

Apache Spark + Hadoop HDFS

Spark == Processing Layer

Hadoop == Storage Layer

##### Example
<code>
	flu_stories = news_json.map(lambda x: x['fulltext']).filter(lambda x: " flu " in x)
	flu_stories.cache()
	print '# of flu stories', flu_stories.count()


	word_count = flu_stories,flatMap(lambda x: x.split()).map(lambda x: (x,1)).reduceByKey(lambda x,y: x+y)
	word_count = word_count.sortBy(lambda x:x[1], ascending=False)
	word_count.take(10)
</code>

map()
filter()







